15:08:37-INFO: device: cuda n_gpu: 1, distributed training: False, 16-bits training: False
15:08:37-INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /h/zhaostep/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
15:08:38-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /h/zhaostep/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
15:08:38-INFO: extracting archive file /h/zhaostep/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpe857r6v7
15:08:42-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

15:08:45-INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
15:08:45-INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
15:08:49-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /h/zhaostep/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
15:08:49-INFO: extracting archive file /h/zhaostep/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpai2q237s
15:08:54-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

15:08:56-INFO: ***** Running evaluation *****
15:08:56-INFO:   Num examples = 1043
15:08:56-INFO:   Batch size = 32
Evaluating:   0%|          | 0/33 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/33 [00:00<00:16,  1.89it/s]Evaluating:   6%|▌         | 2/33 [00:00<00:14,  2.20it/s]Evaluating:   9%|▉         | 3/33 [00:01<00:12,  2.49it/s]Evaluating:  12%|█▏        | 4/33 [00:01<00:10,  2.74it/s]Evaluating:  15%|█▌        | 5/33 [00:01<00:09,  2.94it/s]Evaluating:  18%|█▊        | 6/33 [00:01<00:08,  3.11it/s]Evaluating:  21%|██        | 7/33 [00:02<00:08,  3.23it/s]Evaluating:  24%|██▍       | 8/33 [00:02<00:07,  3.33it/s]Evaluating:  27%|██▋       | 9/33 [00:02<00:07,  3.40it/s]Evaluating:  30%|███       | 10/33 [00:03<00:06,  3.45it/s]Evaluating:  33%|███▎      | 11/33 [00:03<00:06,  3.48it/s]Evaluating:  36%|███▋      | 12/33 [00:03<00:05,  3.51it/s]Evaluating:  39%|███▉      | 13/33 [00:03<00:05,  3.53it/s]Evaluating:  42%|████▏     | 14/33 [00:04<00:05,  3.54it/s]Evaluating:  45%|████▌     | 15/33 [00:04<00:05,  3.55it/s]Evaluating:  48%|████▊     | 16/33 [00:04<00:04,  3.54it/s]Evaluating:  52%|█████▏    | 17/33 [00:05<00:04,  3.55it/s]Evaluating:  55%|█████▍    | 18/33 [00:05<00:04,  3.56it/s]Evaluating:  58%|█████▊    | 19/33 [00:05<00:03,  3.56it/s]Evaluating:  61%|██████    | 20/33 [00:05<00:03,  3.56it/s]Evaluating:  64%|██████▎   | 21/33 [00:06<00:03,  3.57it/s]Evaluating:  67%|██████▋   | 22/33 [00:06<00:03,  3.57it/s]Evaluating:  70%|██████▉   | 23/33 [00:06<00:02,  3.57it/s]Evaluating:  73%|███████▎  | 24/33 [00:06<00:02,  3.57it/s]Evaluating:  76%|███████▌  | 25/33 [00:07<00:02,  3.57it/s]Evaluating:  79%|███████▉  | 26/33 [00:07<00:01,  3.57it/s]Evaluating:  82%|████████▏ | 27/33 [00:07<00:01,  3.57it/s]Evaluating:  85%|████████▍ | 28/33 [00:08<00:01,  3.57it/s]Evaluating:  88%|████████▊ | 29/33 [00:08<00:01,  3.57it/s]Evaluating:  91%|█████████ | 30/33 [00:08<00:00,  3.57it/s]Evaluating:  94%|█████████▍| 31/33 [00:08<00:00,  3.57it/s]Evaluating:  97%|█████████▋| 32/33 [00:09<00:00,  3.57it/s]Evaluating: 100%|██████████| 33/33 [00:09<00:00,  3.95it/s]Evaluating: 100%|██████████| 33/33 [00:09<00:00,  3.51it/s]
15:09:05-INFO: ***** Eval results *****
15:09:05-INFO:   Matthew's correlation = 0.5753011877541249
15:09:05-INFO:   eval_accuracy = 0.8274209012464045
15:09:05-INFO:   eval_loss = 0.4914660221247962
15:09:05-INFO:   global_step = 0
15:09:05-INFO:   inference_time = 9.37225866317749
15:09:05-INFO:   loss = None
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
Base acc: 0.5753011877541249
Part: 
Layer: 1	-0.01021	0.00000	0.00006	0.00252	0.00013	-0.00234	-0.00757	0.00006	0.00505	-0.00507	0.00000	-0.00250Layer: 2	0.00000	-0.00513	-0.00250	-0.00250	0.00250	-0.00250	-0.00250	0.00000	-0.00250	-0.00250	-0.00243	-0.00516Layer: 3	-0.00513	-0.00006	0.00000	-0.00250	-0.00256	0.00000	0.00766	0.00013	-0.00243	-0.01027	0.00262	0.00766Layer: 4	0.00511	-0.00507	0.00006	0.00505	0.00000	-0.00770	-0.00256	-0.01014	-0.00250	0.00760	0.01021	-0.00513Layer: 5	-0.00250	0.00252	-0.00770	0.00511	0.00006	0.01267	0.01017	0.00517	0.00006	0.00252	0.00256	-0.00507Layer: 6	0.00252	0.00000	-0.00250	-0.00243	-0.00256	-0.00747	0.00000	0.00006	0.00252	0.00505	-0.00250	0.00006Layer: 7	0.00262	0.00000	-0.00516	-0.01285	0.00760	0.00511	-0.00250	-0.00250	-0.00250	-0.00516	-0.00262	0.00507Layer: 8	0.00256	-0.00516	-0.00260	0.00000	-0.00507	0.00772	0.00762	-0.00250	0.00262	0.00256	0.01017	-0.00256Layer: 9	-0.00256	-0.00250	0.00000	0.00000	0.00000	0.00250	-0.00001	0.00006	-0.02309	0.00006	0.00262	0.00006Layer: 10	0.00000	0.00000	0.00000	0.00505	0.00000	0.00252	0.00000	0.00000	-0.00500	0.00000	0.00250	0.00006Layer: 11	0.00252	0.00000	0.00000	-0.00250	0.00000	0.00252	-0.00250	0.00252	0.00000	-0.00250	0.00000	0.00000Layer: 12	0.00000	0.00252	0.00000	0.00000	0.00252	0.00000	0.00252	0.00000	0.00000	0.00000	0.00000	0.00000